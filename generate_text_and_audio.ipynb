{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pIZ3ZXNp7cf"
   },
   "source": [
    "## Welcome to Stockastic Piraites! üè¥‚Äç‚ò†Ô∏èüè¥‚Äç‚ò†Ô∏èüè¥‚Äç‚ò†Ô∏è ü¶ú\n",
    "\n",
    "### Libraries versions\n",
    "Correct version of torch is important!\n",
    "- pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
    "- pip3 install transformers==4.31.0\n",
    "- pip install ctransformers\n",
    "- pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0.dev20231218+cu121\n",
      "CUDA is available in your PyTorch installation.\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Print the result\n",
    "if cuda_available:\n",
    "    print(\"CUDA is available in your PyTorch installation.\")\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available in your PyTorch installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Replace 'your_pid_here' with the actual PID of the process you want to stop\n",
    "#process_pid = 1175331\n",
    "\n",
    "# Call the function to stop the process\n",
    "#stop_process(process_pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart Kernel\n",
    "restart = True\n",
    "if restart:\n",
    "    import IPython\n",
    "    #IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel\n",
    "    IPython.Application.instance().kernel.do_restart() #does not automatically restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/tortoise/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda/envs/tortoise/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n"
     ]
    }
   ],
   "source": [
    "# Imports used through the rest of the notebook.\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import IPython\n",
    "\n",
    "from tortoise.api import TextToSpeech\n",
    "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "#preset super_fast, fast, standard, high_quality\n",
    "preset = \"fast\"\n",
    "\n",
    "# This will download all the models used by Tortoise from the HF hub.\n",
    "tts = TextToSpeech(use_deepspeed=False, kv_cache=True, device='cuda')\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"/workspace/data/mistral_model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is Monday\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def get_day_of_week():\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.datetime.now()\n",
    "\n",
    "    # Use the strftime method to format the date and extract the day of the week\n",
    "    day_of_week = current_datetime.strftime(\"%A\")\n",
    "\n",
    "    return day_of_week\n",
    "\n",
    "# Call the function and print the result\n",
    "day = get_day_of_week()\n",
    "print(\"Today is\", day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics 6\n"
     ]
    }
   ],
   "source": [
    "#Create a list of topics\n",
    "generalTopics = [\"God\", \"chat G.P.T. religion\", \"quantum physics\", \"A.I. will make human useless\", \"descoveries in the human brain\", \"close the program\"]\n",
    "\n",
    "listOFTopics = [\"They talk about if god exist or not, for instance if God created the universe and god is omniscient, omnipotent and omnibenevolent, why is needed a simulation? make it funny\",\n",
    "                \"They talk about chat G.P.T. inventing a new religion that all humans are going to follow blindly\",\n",
    "                \"They explain quantum physics for advanced audience, use complex terms\",\n",
    "                \"They talk about A.I. will make humans useless, for instance, if A.I. can do everything, why do we need humans?\",\n",
    "                \"They talk about recent descoveries in the human brain, make it funny\",\n",
    "                \"They close the program, make it funny\"]\n",
    "\n",
    "print(\"number of topics\", len(listOFTopics))\n",
    "preTopic = \"Generate a radio conversation of two commentators, Anna and Phillip. \"\n",
    "topic = \"\"\n",
    "postTopic = \". Don't bring more people into the conversation, only two voices. Only refers to them with their names, avoid adding time or other elements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the conversations and save them in the data folder\n",
    "from tqdm import tqdm\n",
    "\n",
    "mistral_generation = \"\"\n",
    "changeTopic = \"\"\n",
    "day = get_day_of_week()\n",
    "\n",
    "for i in tqdm(range(len(listOFTopics))):\n",
    "    print(i, \"\\n--------------------------------------\")\n",
    "    if i == 0:\n",
    "        changeTopic = \"Start with greetings too all the audience, knowing that today is \" + day + \". \"\n",
    "    else:\n",
    "        changeTopic = \"No greetings, don't say hey. They were talking about \" + generalTopics[i-1] + \" and now they change to \" + generalTopics[i] + \". \"\n",
    "    prompt =  preTopic + changeTopic + listOFTopics[i] + postTopic\n",
    "    print(prompt+\"\\n\")\n",
    "    generate=llm(prompt)\n",
    "    #print(generate, \"\\n\")\n",
    "    mistral_generation += generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix some text errors \n",
    "replacements = {\n",
    "    \"Anna:\": \"\\nAnna:\",\n",
    "    \"Phillip:\": \"\\nPhillip:\",\n",
    "    \"(laughing)\": \"hahahaha\",\n",
    "    \"(laugh)\": \"hahaha\",\n",
    "    \"(laughs)\": \"hahaha\",\n",
    "    \"(smiling)\": \"hahaa\",\n",
    "    \"(smile)\": \"hahaha\",\n",
    "    \"(smiles)\": \"hahaha\",\n",
    "    \"(chuckles)\": \"haha\",\n",
    "    \"\\n\\n\": \"\\n\",\n",
    "}\n",
    "#Fix expressions \n",
    "final_text = mistral_generation\n",
    "for old, new in replacements.items():\n",
    "    final_text = final_text.replace(old, new)\n",
    "\n",
    "final_text = final_text.replace(\"\\n\\n\", \"\\n\")\n",
    "# Split the text into lines and get the first line\n",
    "lines = final_text.splitlines()\n",
    "\n",
    "# Check if the first line starts with \"Anna:\" or \"Phillip:\"\n",
    "if lines and (lines[0].startswith(\"Anna:\") or lines[0].startswith(\"Phillip:\")):\n",
    "    # Print the first line\n",
    "    print(\"First line error fixed:\", lines[0])\n",
    "else:\n",
    "    # Erase the first line\n",
    "    lines.pop(0)\n",
    "\n",
    "# Print the modified text (without the erased line)\n",
    "final_text = '\\n'.join(lines)\n",
    "\n",
    "#print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateVoice(voice, text, preset, fileName):\n",
    "    # Pick one of the voices from the output above\n",
    "    # Load it and send it through Tortoise.\n",
    "    voice_samples, conditioning_latents = load_voice(voice)\n",
    "    gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)\n",
    "    torchaudio.save(fileName, gen.squeeze(0).cpu(), 24000)\n",
    "    print(\"Saved to \" + fileName)\n",
    "    #IPython.display.Audio(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "def getCurrentTime():\n",
    "    current_datetime = datetime.now()\n",
    "\n",
    "    # Extract the date, hour, minute, and second components\n",
    "    current_date = current_datetime.strftime(\"%d%m%y\")\n",
    "    current_hour = current_datetime.hour\n",
    "    current_minute = current_datetime.minute\n",
    "    current_second = current_datetime.second\n",
    "\n",
    "    # Calculate the current second of the day\n",
    "    current_second_of_day = current_hour * 3600 + current_minute * 60 + current_second\n",
    "\n",
    "    # Create a unique ID for naming files\n",
    "    file_id = f\"{current_second_of_day}_{current_date}\"\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the audio samples\n",
    "def audioGeneration(voice, text, preset):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"voice presset:\", voice, \"\\n\", text, \"\\n\")\n",
    "    path = \"../workspace/data/presenters/\"\n",
    "    time = getCurrentTime()\n",
    "    fileName =  path + time +\"_\"+ voice + '.wav'\n",
    "    generateVoice(voice, text, preset, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 15 files from /workspace/data/presenters...\n",
      "Successfully deleted /workspace/data/presenters/45724_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/45797_181223_tom.wav\n",
      "Successfully deleted /workspace/data/presenters/45857_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/45957_181223_tom.wav\n",
      "Successfully deleted /workspace/data/presenters/46052_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/46129_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/46166_181223_tom.wav\n",
      "Successfully deleted /workspace/data/presenters/46248_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/46276_181223_tom.wav\n",
      "Successfully deleted /workspace/data/presenters/46363_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/46403_181223_tom.wav\n",
      "Successfully deleted /workspace/data/presenters/46523_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/46553_181223_emma.wav\n",
      "Successfully deleted /workspace/data/presenters/46608_181223_tom.wav\n",
      "Successfully deleted /workspace/data/presenters/46729_181223_emma.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to the 'data' folder\n",
    "folder_path = '/workspace/data/presenters'\n",
    "\n",
    "# Check if the folder exists before attempting to remove its contents\n",
    "if os.path.exists(folder_path):\n",
    "    # Get a list of all files in the 'data' folder\n",
    "    files = os.listdir(folder_path)\n",
    "    print(f\"Removing {len(files)} files from {folder_path}...\")\n",
    "    # Iterate through the files and remove each one\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Successfully deleted {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "else:\n",
    "    # If the folder doesn't exist, create it\n",
    "    os.makedirs(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split text into two sections dictionary, one for Anna and one for Phillip\n",
    "lines = final_text.split('\\n')\n",
    "\n",
    "# Flag to determine the current speaker\n",
    "current_speaker = None\n",
    "\n",
    "# Iterate through lines and populate dictionaries\n",
    "for line in lines:\n",
    "    if line.startswith(\"Anna:\"):\n",
    "        current_speaker = \"angie\" #this is the voice name in the dataset\n",
    "        text = line[len(\"anna:\"):].strip()\n",
    "        audioGeneration(current_speaker, text, preset)\n",
    "        \n",
    "    elif line.startswith(\"Phillip:\"):\n",
    "        current_speaker = \"tom\" #this is the voice name in the dataset\n",
    "        text = line[len(\"phillip:\"):].strip()\n",
    "        audioGeneration(current_speaker, text, preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/workspace/data/presenters/\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Print the list of file names\n",
    "# print(\"File names in the folder:\")\n",
    "# for file_name in file_names:\n",
    "#     print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='../workspace/data/rendered_audio/concatenated_output_1.mp3'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "\n",
    "sourceFolder = \"../workspace/data/rendered_audio/\"\n",
    "\n",
    "audio_segment = AudioSegment.silent(duration=0)\n",
    "# Create an empty audio segment to hold the concatenated audio\n",
    "concatenated_audio = AudioSegment.silent(duration=4000)\n",
    "\n",
    "# Iterate through each audio file and concatenate it\n",
    "for file in file_names:\n",
    "    audio_segment = AudioSegment.from_file(folder_path+file, format=\"wav\")\n",
    "    # Normalize the audio to increase volume\n",
    "    audio_segment = normalize(audio_segment)\n",
    "    concatenated_audio += audio_segment\n",
    "\n",
    "# Export the concatenated audio to a new file\n",
    "concatenated_audio.export(sourceFolder+\"concatenated_output_1.mp3\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='../workspace/data/audio_background/new_background.mp3'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathBackground = \"../workspace/data/audio_background/\"\n",
    "\n",
    "# Load your audio tracks\n",
    "track0 = AudioSegment.from_file(pathBackground+\"opening_news.mp3\", format=\"mp3\")\n",
    "track1 = AudioSegment.from_file(pathBackground+\"Pop.mp3\", format=\"mp3\")\n",
    "track2 = AudioSegment.from_file(pathBackground+\"japanese_pop.mp3\", format=\"mp3\")\n",
    "track3 = AudioSegment.from_file(pathBackground+\"cumbia_3.mp3\", format=\"mp3\")\n",
    "\n",
    "# Add the tracks together\n",
    "combined_track = track0 + track1 + track2 + track3 + track0\n",
    "\n",
    "# Reduce the volume to half with a ramp of 1 second after 2 seconds\n",
    "ramp_duration = 2000  # 1 second in milliseconds\n",
    "start_time = 2000  # 2 seconds in milliseconds\n",
    "\n",
    "# Extract the part of the audio before the volume reduction\n",
    "audio_before_ramp = combined_track[:start_time]\n",
    "\n",
    "# Extract the part of the audio during the volume reduction and apply fade-out\n",
    "audio_ramp = combined_track[start_time:start_time + ramp_duration].fade_out(ramp_duration)\n",
    "\n",
    "# Extract the part of the audio after the volume reduction\n",
    "audio_after_ramp = combined_track[start_time + ramp_duration:]\n",
    "\n",
    "# Decrease the gain by 12 dB to reduce volume to half\n",
    "audio_half_volume = audio_after_ramp - 12\n",
    "\n",
    "# Combine the audio segments\n",
    "final_audio = audio_before_ramp + audio_ramp + audio_half_volume \n",
    "\n",
    "# Export the modified audio to a new file\n",
    "final_audio.export(pathBackground+\"new_background.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix the background music with the conversation\n",
    "background_music = AudioSegment.from_file(pathBackground+\"new_background.mp3\", format=\"mp3\")\n",
    "\n",
    "mixed_audio = concatenated_audio.overlay(background_music)  # Add more overlay calls for additional tracks\n",
    "# Export the mixed audio to a new file\n",
    "time = getCurrentTime()\n",
    "fileName = sourceFolder+time+\"_mixed_output.mp3\"\n",
    "mixed_audio.export(fileName, format=\"wav\")\n",
    "print(fileName)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tortoise-tts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
